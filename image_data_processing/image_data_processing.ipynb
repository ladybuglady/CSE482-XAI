{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 04:46:45.532625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import concatenate,Activation,Dropout,Dense,ZeroPadding2D\n",
    "from keras.layers import Input,add,Conv2D, MaxPooling2D,Flatten,BatchNormalization,LSTM\n",
    "import os\n",
    "import zipfile\n",
    "from pandas import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import *\n",
    "import json\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import numpy_indexed as npi\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to turn the ECG data into image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make images, source and destination file\n",
    "\n",
    "def make_images(ecgs, dest):###\n",
    "\n",
    "    # from frequency values of -2 to 2. 4-decimal point vals in between. So thats like going from -2000 to 2000.\n",
    "    images = np.zeros(10000, 2, 5000, 4000) # 4000 pixels per time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved files:  0\n",
      "Retrieved files:  1000\n",
      "Retrieved files:  2000\n",
      "Retrieved files:  3000\n",
      "Retrieved files:  4000\n",
      "Retrieved files:  5000\n",
      "Retrieved files:  6000\n",
      "Retrieved files:  7000\n",
      "Retrieved files:  8000\n",
      "Retrieved files:  9000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() takes from 1 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     36\u001b[0m     jsonFile\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> 38\u001b[0m make_images(X, \u001b[39m\"\u001b[39;49m\u001b[39mmeow\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mmake_images\u001b[0;34m(ecgs, dest)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_images\u001b[39m(ecgs, dest):\u001b[39m###\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[39m# from frequency values of -2 to 2. 4-decimal point vals in between. So thats like going from -2000 to 2000.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros(\u001b[39m10000\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m5000\u001b[39;49m, \u001b[39m4000\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() takes from 1 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "afib_path =  '../../../../../../../local1/CSE_XAI/study60_recordings_json/'\n",
    "control_path = '../../../../../../../local1/CSE_XAI/control_small/'\n",
    "\n",
    "refs = pd.read_csv(\"../Copy of study60_patient_recordings.csv\")\n",
    "sr_episodes = refs[refs[\"determination\"]==\"Sinus Rhythm\"][\"recording_public_id\"]\n",
    "\n",
    "afib_recordings = random.sample(list(sr_episodes), 10000)\n",
    "afib_recordings = np.array(list(map(lambda s: s+\"_raw.json\", afib_recordings)))\n",
    "afib_ecgs =  np.asarray(os.listdir(afib_path))\n",
    "afib_ecgs = afib_ecgs[npi.indices(afib_ecgs, afib_recordings)]\n",
    "\n",
    "X = np.zeros((10000, 2, 5000))\n",
    "counter = 0\n",
    "for file in afib_ecgs:\n",
    "    if counter %1000 == 0:\n",
    "        print(\"Retrieved files: \", counter)\n",
    "    patient_X = np.empty((2, 5000))\n",
    "\n",
    "    try:\n",
    "        jsonFile = open(afib_path + file, 'r')\n",
    "    except:\n",
    "        jsonFile = open(control_path + file, 'r')\n",
    "\n",
    "    fileContents = json.load(jsonFile)\n",
    "\n",
    "    # digging into the dictionaries to get lead data\n",
    "    lead_1_samples = fileContents['samples']\n",
    "    lead_2_samples = fileContents['extraLeads'][0]['samples']\n",
    "\n",
    "    # Crop the data to 5000 data points.\n",
    "    patient_X[0,:] = lead_1_samples[0:5000]\n",
    "    patient_X[1,:] = lead_2_samples[0:5000]\n",
    "\n",
    "    X[counter] = patient_X\n",
    "    counter += 1\n",
    "    jsonFile.close()\n",
    "\n",
    "make_images(X, \"meow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
